{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import inception_v3 as inc_net\n",
    "from keras.preprocessing import image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "from pathlib import Path\n",
    "from rembg import remove, new_session\n",
    "from lime import lime_image\n",
    "\n",
    "import sys\n",
    "sys.path.append('../functions')\n",
    "\n",
    "RSEED = 42\n",
    "DATASET_PATH = '../data/images/' # Path to the parent folder where the original data is stored\n",
    "TRAINING_IMAGES = ''\n",
    "TESTING_IMAGES = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Consolidating classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon closer inspection it became apparent that there are some classes with very bad images (=images that the model didn't train well on) or classes that should either be excluded or grouped together based on domain knowledge of crop diseases. Since the aim of this project is to provide farmers with treatments for the diseases, we'll also exclude diseases that are too broad to recommend any treatment for.\n",
    "\n",
    "Removed classes:\n",
    "- scab (bad training data)\n",
    "- green_mottle (bad training data)\n",
    "- gray_spot_rust (not specific enough for remedy)\n",
    "- yellow_leaf (condition not disease)\n",
    "- leaf_curl (condition not disease)\n",
    "- leaf_blight\n",
    "- leaf_scorch\n",
    "- pests (too unspecific)\n",
    "- nematode\n",
    "- virus (too unspecific)\n",
    "\n",
    "Merged classes:\n",
    "- septoria has been merged with brown_spot\n",
    "- phytophora has been merged with late_blight\n",
    "- mosaic_disease has been merged with mosaic_virus\n",
    "\n",
    "This reduced the final classes to 25 (24 diseases and 1 healthy class)\n",
    "> alternaria_leaf_spot, bacterial_blight, bacterial_spot, bacterial_wilt, black_measles, black_rot, blast, brown_spot, brown_streak_disease, citrus_greening common_rust, early_blight, gray_leaf_spot, healthy, isariopsis_leaf_spot, late_blight, leaf_curl, leaf_mold, mosaic_disease, northern_leaf_blight, powdery_mildew,red_rot, spider_mites, target_spot, tungro\n",
    "\n",
    "*We've removed and copied the classes manually, so there's no one-step solution to reproduce this step.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image augmentation can make a model more robust and prevent overfitting by introducing more variance by altering the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code first defines the augmentations to perform on the training and validation data. The validation data only gets rescaled.\n",
    "\n",
    "batch_size = 32\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=10,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.2)  # val 20%\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(TRAINING_IMAGES, \n",
    "                                               target_size=(224, 224), \n",
    "                                               color_mode='rgb',\n",
    "                                               batch_size=batch_size, \n",
    "                                               class_mode='categorical',\n",
    "                                               shuffle=True,\n",
    "                                               subset = 'training') \n",
    "\n",
    "val_data = val_datagen.flow_from_directory(TRAINING_IMAGES, \n",
    "                                           target_size=(224, 224), \n",
    "                                           color_mode='rgb',\n",
    "                                           batch_size=batch_size, \n",
    "                                           class_mode='categorical',\n",
    "                                           shuffle=False,\n",
    "                                           subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code to display and check the augmented images\n",
    "# show augmented images for training\n",
    "\n",
    "# Get the first batch of images and labels\n",
    "batch = next(train_data)\n",
    "\n",
    "# Extract images from the batch\n",
    "images = batch[0]\n",
    "\n",
    "# Plot multiple images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(20):  # Adjust the number of images to display\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# show augmented images for validation\n",
    "\n",
    "# Get the first batch of images and labels\n",
    "batch = next(val_data)\n",
    "\n",
    "# Extract images from the batch\n",
    "images = batch[0]\n",
    "\n",
    "# Plot multiple images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(4):  # Adjust the number of images to display\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didn't help our model, so next we've had a look at what the model learned to be important using the explainer lime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Background removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what the model bases its predictions on, or which parts of the images are most relevant to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Image explainer (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code uses lime explainer on a sample image\n",
    "\n",
    "IMAGE = '../data/external_test_data/bacterial_spot/bacterial-symptoms-pepper.jpg' # the image to be tested\n",
    "MODEL = keras.models.load_model('../models/model_filtered.h5') # the model to be used for making predictions\n",
    "CLASSES = [\n",
    "    'alternaria_leaf_spot',\n",
    "    'bacterial_blight',\n",
    "    'bacterial_spot',\n",
    "    'bacterial_wilt',\n",
    "    'black_measles',\n",
    "    'black_rot',\n",
    "    'blast',\n",
    "    'brown_spot',\n",
    "    'brown_streak_disease',\n",
    "    'citrus_greening',\n",
    "    'common_rust',\n",
    "    'early_blight',\n",
    "    'gray_leaf_spot',\n",
    "    'healthy',\n",
    "    'isariopsis_leaf_spot',\n",
    "    'late_blight',\n",
    "    'leaf_curl',\n",
    "    'leaf_mold',\n",
    "    'mosaic_disease',\n",
    "    'northern_leaf_blight',\n",
    "    'powdery_mildew',\n",
    "    'red_rot',\n",
    "    'spider_mites',\n",
    "    'target_spot',\n",
    "    'tungro',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_img_fn(path_list):\n",
    "    out = []\n",
    "    for img_path in path_list:\n",
    "        img = image.load_img(img_path, target_size=(224, 224))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = inc_net.preprocess_input(x)\n",
    "        out.append(x)\n",
    "    return np.vstack(out)\n",
    "\n",
    "images = transform_img_fn([os.path.join(IMAGE)])\n",
    "plt.imshow(images[0] / 2 + 0.5)\n",
    "preds = MODEL.predict(images)\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(images[0].astype('double'), MODEL.predict, top_labels=5, hide_color=0, num_samples=1000)\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=4, hide_rest=False)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=4, hide_rest=False)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "\n",
    "images = transform_img_fn([os.path.join(IMAGE)])\n",
    "preds = MODEL.predict(images)\n",
    "\n",
    "def display_class_probabilities(model, img_path, class_names):\n",
    "    # Load and preprocess the input data\n",
    "    img = image.load_img(img_path, target_size=(224, 224)) \n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    # Get class probabilities\n",
    "    probabilities = model.predict(img_array)[0]\n",
    "\n",
    "# Get indices of the top 3 predicted classes\n",
    "    top3_indices = np.argsort(probabilities)[::-1][:3]\n",
    "\n",
    "    # Display top 3 predicted classes with probabilities\n",
    "    print(\"Top 3 Predicted Classes:\")\n",
    "    count = 1\n",
    "    for i in top3_indices:\n",
    "        print(f\"{count}. {class_names[i]}: {probabilities[i]}\")\n",
    "        count += 1\n",
    "\n",
    "display_class_probabilities(MODEL, IMAGE, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the model targets large proportions of the background, so we tried how things look without a background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Background removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = new_session()\n",
    "\n",
    "# Define the input directory\n",
    "input_directory = ''\n",
    "\n",
    "# Define the output directory\n",
    "output_directory = ''\n",
    "\n",
    "# Walk through all directories and subdirectories\n",
    "for x in next(os.walk(input_directory))[1]:\n",
    "    for filename in os.listdir(os.path.join(input_directory, x)):\n",
    "        # Construct the full file path for input and output files\n",
    "        input_filepath = os.path.join(input_directory, x, filename)\n",
    "        output_filepath = os.path.join(output_directory, x, filename)\n",
    "        # Ensure the output directory exists, if not, create it\n",
    "        os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "        # Convert to Path objects for convenience\n",
    "        input_path = Path(input_filepath)\n",
    "        output_path = Path(output_filepath)\n",
    "        # Check if the output file already exists\n",
    "        if output_path.is_file():\n",
    "            print(f\"Output file {output_path} already exists. Skipping...\")\n",
    "            continue\n",
    "        # Open the input image\n",
    "        with open(input_filepath, 'rb') as input_file:\n",
    "            input_data = input_file.read()\n",
    "            # Perform your operation (e.g., using remove function)\n",
    "            output_data = remove(input_data, session=session)\n",
    "        # Write the processed data to the output file\n",
    "        with open(output_filepath, 'wb') as output_file:\n",
    "            output_file.write(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of removing backgrounds and converting the resulting images to .png files with alpha-layer proved to be rather time-consuming. We tried it on a small test smaple and since the explainer still marked large parts of the (now transparent) background as relevant, we omitted this approach for the time being. Another reason why we abandoned this technique is because automatically removing the background of some images rendered them almost empty and thus unrecognizable. One would have to spend a lot of time to sort those images out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step to further fine tune the model was to unfreeze some more layers of the pre-trained model and retrain them with our own data. This lead to a better model so we include this in the training of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to retrain a model and unfreeze the top 10 layers\n",
    "\n",
    "def unfreeze_model_and_clone(model):\n",
    "    # Clone the original model\n",
    "    unfrozen_model = tf.keras.models.clone_model(model)\n",
    "    unfrozen_model.set_weights(model.get_weights())  # Copy weights\n",
    "\n",
    "    # Unfreeze the top 10 layers while leaving BatchNorm layers frozen\n",
    "    for layer in unfrozen_model.layers[-10:]:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5)\n",
    "    unfrozen_model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return unfrozen_model\n",
    "\n",
    "# Create a new model with unfrozen layers\n",
    "unfrozen_model = unfreeze_model_and_clone(model)\n",
    "\n",
    "epochs = 2\n",
    "hist = unfrozen_model.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
    "\n",
    "# Save the model to disk\n",
    "unfrozen_model.save(\"unfrozen_model.h5\")\n",
    "\n",
    "# Check performance of the unfrozen model\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(hist)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
